{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "from skimage.metrics import structural_similarity as SSIM\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import PIL\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy.ma as ma\n",
    "from skimage.metrics import structural_similarity as SSIM\n",
    "from WGAIN_model import *\n",
    "print(tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.io.read_file('./images/cat.png')\n",
    "img = tf.image.decode_png(img, channels=1)\n",
    "img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "img = tf.image.resize(img, (117, 117))\n",
    "img = ma.masked_greater(img, 0)\n",
    "custom_img=img.mask.astype(int)\n",
    "custom_img = 1 - custom_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "def prepare_noise_eval_dataset(ds, batch_size, side, throw_ratio):\n",
    "    def introduce_missingness(X):\n",
    "        # per row ratios how much should be thrown (each row a different number, max is max_ratio_to_throw)\n",
    "        ratio_to_throw = throw_ratio*tf.ones([X.shape[0], side, side, 1])\n",
    "        # binary mask of which features are left (0 - missing, 1 - OK)\n",
    "        mask = tf.dtypes.cast(tf.math.greater(tf.random.uniform([X.shape[0], side, side, 1]), ratio_to_throw), tf.float32)\n",
    "        # Do the mask to have 3 channels\n",
    "        mask3 = tf.repeat(mask, [3], axis = 3)\n",
    "        # prepare randomness\n",
    "        randZ = tf.random.normal([X.shape[0], side, side, 3], mean = 0.0, stddev = 0.1)\n",
    "        randZ = tf.math.multiply(1 - mask3, randZ)\n",
    "        # apply the mask - leave original and substitute randoms for missings\n",
    "        newX = tf.math.multiply(mask3, X)\n",
    "        # return a training pair\n",
    "        return (X, newX, mask, randZ)\n",
    "    # do the preparation\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size,drop_remainder = True)\n",
    "    ds = ds.map(introduce_missingness, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "def prepare_singlesquare_eval_dataset(ds, batch_size, side):\n",
    "    def introduce_missingness(X):\n",
    "        # centered square\n",
    "        sides = np.ones(shape=(X.shape[0],))*int(side/2.0)\n",
    "        corner_l = int(side / 2.0) -  (sides/2.0).astype(int)\n",
    "        corner_u = int(side / 2.0) +  (sides/2.0).astype(int)\n",
    "        center_mask = np.ones((X.shape[0], side, side, 1)).astype('float32')\n",
    "        for i in range(X.shape[0]):\n",
    "            center_mask[i,corner_l[i]:corner_u[i],corner_l[i]:corner_u[i],:] = 0\n",
    "        # binary mask of which features are left (0 - missing, 1 - OK)\n",
    "        mask = tf.constant(center_mask, dtype=tf.float32)\n",
    "        # Do the mask to have 3 channels\n",
    "        mask3 = tf.repeat(mask, [3], axis = 3)\n",
    "        # prepare randomness\n",
    "        randZ = tf.random.normal([X.shape[0], side, side, 3], mean = 0.0, stddev = 0.1)\n",
    "        randZ = tf.math.multiply(1 - mask3, randZ)\n",
    "        # apply the mask - leave original and substitute randoms for missings\n",
    "        newX = tf.math.multiply(mask3, X)\n",
    "        # return a training pair\n",
    "        return (X, newX, mask, randZ)\n",
    "    # do the preparation\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size,drop_remainder = True)\n",
    "    ds = ds.map(introduce_missingness, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def prepare_multisquare_eval_dataset(ds, batch_size, side):\n",
    "    def introduce_missingness(X):\n",
    "        # multi square\n",
    "        DAMAGE = 0.3\n",
    "        SQUARES = 5.0\n",
    "        image_area = side*side\n",
    "        damage_area = DAMAGE*image_area\n",
    "        one_square_area = damage_area / SQUARES\n",
    "        one_square_side = int(np.sqrt(one_square_area))\n",
    "        test_mask = np.ones((X.shape[0],side,side,1))\n",
    "        for i in range(test_mask.shape[0]):\n",
    "            for k in range(int(SQUARES)):\n",
    "                x = np.random.randint(side - one_square_side)\n",
    "                y = np.random.randint(side - one_square_side)\n",
    "                test_mask[i,x:x+one_square_side,y:y+one_square_side,0] = 0\n",
    "        # binary mask of which features are left (0 - missing, 1 - OK)\n",
    "        mask = tf.constant(test_mask, dtype=tf.float32)\n",
    "        # Do the mask to have 3 channels\n",
    "        mask3 = tf.repeat(mask, [3], axis = 3)\n",
    "        # prepare randomness\n",
    "        randZ = tf.random.normal([X.shape[0], side, side, 3], mean = 0.0, stddev = 0.1)\n",
    "        randZ = tf.math.multiply(1 - mask3, randZ)\n",
    "        # apply the mask - leave original and substitute randoms for missings\n",
    "        newX = tf.math.multiply(mask3, X)\n",
    "        # return a training pair\n",
    "        return (X, newX, mask, randZ)\n",
    "    # do the preparation\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size,drop_remainder = True)\n",
    "    ds = ds.map(introduce_missingness, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "def prepare_customMask_eval_dataset(ds, batch_size, side):\n",
    "    def decode_img(file_path):\n",
    "        img = tf.io.read_file(file_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img\n",
    "    def introduce_missingness(X):\n",
    "        # Custom Mask\n",
    "        corner_l = np.random.randint(low = 5, high = 10, size=batch_size)\n",
    "        corner_u = int(side / 2.0)  +  corner_l + 53\n",
    "        center_mask = np.ones((batch_size, side, side, 1)).astype('float32')\n",
    "        for i in range(batch_size):\n",
    "            center_mask[i,corner_l[i]:corner_u[i],corner_l[i]:corner_u[i],:] = custom_img\n",
    "        mask = tf.constant(center_mask, dtype=tf.float32)\n",
    "        \n",
    "        mask3 = tf.repeat(mask, [3], axis = 3)\n",
    "        # prepare randomness\n",
    "        randZ = tf.random.normal([X.shape[0], side, side, 3], mean = 0.0, stddev = 0.1)\n",
    "        randZ = tf.math.multiply(1 - mask3, randZ)\n",
    "        # apply the mask - leave original and substitute randoms for missings\n",
    "        newX = tf.math.multiply(mask3, X)\n",
    "        # return a training pair\n",
    "        return (X, newX, mask, randZ)\n",
    "    # do the preparation\n",
    "    ds = ds.map(decode_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    #ds = ds.shuffle(buffer_size = 1024)\n",
    "    ds = ds.batch(batch_size,drop_remainder = True)\n",
    "    ds = ds.map(introduce_missingness, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "def evaluate_dataset(dataset, batch_size, side, print_log = True, max_batch = 5):\n",
    "    count = 0\n",
    "    psnr = 0\n",
    "    ssim = 0\n",
    "    for i, (origX, newX, mask, randZ) in enumerate(dataset):\n",
    "        if max_batch and i > max_batch:\n",
    "            break\n",
    "        if print_log:\n",
    "            print(i)\n",
    "        imputed = generator([newX,mask,randZ])\n",
    "        imputed = mask*origX + (1-mask)*imputed\n",
    "        count += imputed.shape[0]\n",
    "        for j in range(imputed.shape[0]):\n",
    "            ssim += SSIM(origX[j,:,:,:].numpy(), imputed[j,:,:,:].numpy(), multichannel=True)\n",
    "            psnr += tf.image.psnr(origX[j,:,:,:], imputed[j,:,:,:], 1)\n",
    "        if print_log:\n",
    "            print(\"sample count:\", count)\n",
    "            print(\"avg test PSNR: \", psnr/count)\n",
    "            print(\"avg test SSIM: \", ssim/count)\n",
    "    return count, psnr/count, ssim/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x240e8bb3b50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "side = 128\n",
    "\n",
    "path = \"./datasets/celeba/%s128\"\n",
    "celeba_test = data.Dataset.list_files(path%'test' + str('/*.jpg'))\n",
    "celeba_test_d = prepare_multisquare_eval_dataset(celeba_test, batch_size, side)\n",
    "\n",
    "# create the model\n",
    "generator = build_generator(side)\n",
    "# generator.summary()\n",
    "critic = build_critic(side)\n",
    "# critic.summary()\n",
    "\n",
    "# Load weights\n",
    "model_path = \"saved_models_celeba_old/\"\n",
    "\n",
    "generator.load_weights(model_path + \"gultimate_final\")\n",
    "critic.load_weights(model_path + \"cultimate_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center\n",
      "96 tf.Tensor(26.315775, shape=(), dtype=float32) 0.9238268312184464\n",
      "multi\n",
      "96 tf.Tensor(27.293007, shape=(), dtype=float32) 0.9305722618792864\n",
      "noise 50\n",
      "96 tf.Tensor(34.03396, shape=(), dtype=float32) 0.9769218633283111\n",
      "noise 75\n",
      "96 tf.Tensor(30.140223, shape=(), dtype=float32) 0.9479118590056109\n",
      "noise 95\n",
      "96 tf.Tensor(23.983976, shape=(), dtype=float32) 0.8364108153944247\n",
      "customMask\n",
      "96 tf.Tensor(26.18847, shape=(), dtype=float32) 0.9187091383006473\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "psnrs = []\n",
    "ssims = []\n",
    "scenarios = ['center', 'multi', 'noise 50', 'noise 75', 'noise 95', 'customMask']\n",
    "datasets = [\n",
    "    prepare_singlesquare_eval_dataset(celeba_test, batch_size, side),\n",
    "    prepare_multisquare_eval_dataset(celeba_test, batch_size, side),\n",
    "    prepare_noise_eval_dataset(celeba_test, batch_size, side, 0.50),\n",
    "    prepare_noise_eval_dataset(celeba_test, batch_size, side, 0.75),\n",
    "    prepare_noise_eval_dataset(celeba_test, batch_size, side, 0.95),\n",
    "    prepare_customMask_eval_dataset(celeba_test, batch_size, side)\n",
    "]\n",
    "for i in range(len(datasets)):\n",
    "    print(scenarios[i])\n",
    "    _count, _psnr, _ssim = evaluate_dataset(datasets[i], batch_size, side, False, max_batch=100)\n",
    "    print(_count, _psnr, _ssim)\n",
    "    counts.append(_count)\n",
    "    psnrs.append(_psnr)\n",
    "    ssims.append(_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
